more than a half a century before he was Stephen Hawking's and Elon Musk felt compelled to won the world of artificial intelligence back in 1942 before the term was even coined the science fiction writer Isaac Asimov wrote the three laws of robotics a moral code to keep our machines in check and the three laws of robotics are
a robot may not injure a human being or through an action allow a human being to come to harm
the second law a robot must obey order is given by human beings except for such orders with conflict with the first law
and the third a robot must protect its own existence as long as such protection does not comply with the first and the second law that sounds logical disease Three Laws provide a basis to work from to develop more robots Marcus what you think
did they make for good science fiction there lots of clots the turnaround having these kind of lost but the first problem if you never programmed anything is like arm is really hard to program into machines that's one thing that program in Geometry or compound interest or something like that where we have precise necessary and sufficient conditions nobody has any idea how to generalize we get a machine to recognize something like armor Justice is it was very serious programming problem then there are a couple other problems to one is that not everybody would agree that the robot should never allow humans come to harm and what if what if we resemble were talking about a terrorist sniper or something like that I mean some people not everybody but some people might actually want to allow that into the what they wouldn't let robots do and then the third issue he really think through the third one of those laws is it set up robot to be second-class citizens in ultimately to be slaves and right now that might seem okay because robots don't seem very clever but as they get smarter and smarter they might resent that or it might not feel like the appropriate thing to do
Dino's laws might not be fair to robots he might not be exactly what I'm saying but the problem is not just with the machines but I really do we know what fair is is if we agree we should be fair to robots that's part of the problem is we don't know what code we should program into asimov's laws or nice starting point at least for for a novel but for example imagine that we programmed in our laws from the 17th century then we would have thought slavery was okay so mean maybe don't want to program in the fix the laws that we have right now to Shackle the robot forever we don't burn them into the wrong ships of the robots but we also don't know how we want those Morel to grow over time if it's a very complicated issue